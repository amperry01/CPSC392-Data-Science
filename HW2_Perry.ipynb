{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings to keep the notebook clean (like HW1)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data / utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report, brier_score_loss\n",
    ")\n",
    "\n",
    "# neighbors for the recommendation section\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATA LOADING ===\n",
    "# Update these if your instructor posted different paths.\n",
    "CHURN_URL = \"https://raw.githubusercontent.com/ywen2021/CPSC392/main/Data/streaming.csv\"\n",
    "NEW_CUSTOMERS_URL = \"https://raw.githubusercontent.com/ywen2021/CPSC392/refs/heads/main/Data/streamingNEW.csv\"\n",
    "# Optional: a favorites dataset. If not provided, we'll still do KNN on demographics/usage.\n",
    "FAVORITES_URL = \"https://raw.githubusercontent.com/ywen2021/CPSC392/refs/heads/main/Data/streamingFILMS.csv\"\n",
    "\n",
    "# read data\n",
    "churn = pd.read_csv(CHURN_URL)\n",
    "new_customers = pd.read_csv(NEW_CUSTOMERS_URL)\n",
    "try:\n",
    "    favorites = pd.read_csv(FAVORITES_URL)\n",
    "except Exception:\n",
    "    favorites = None  # if not available, we'll skip film-preference features\n",
    "\n",
    "# quick clean\n",
    "for df in (churn, new_customers):\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# (optional) make sure plan is treated as ordered categorical if it exists\n",
    "if \"plan\" in churn.columns:\n",
    "    plan_order = pd.CategoricalDtype(categories=[\"T\", \"P\", \"A\", \"B\"], ordered=True)\n",
    "    churn[\"plan\"] = churn[\"plan\"].astype(plan_order)\n",
    "    if \"plan\" in new_customers.columns:\n",
    "        new_customers[\"plan\"] = new_customurers_plan = new_customers[\"plan\"].astype(plan_order)\n",
    "\n",
    "churn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE SETUP ===\n",
    "# Use everything EXCEPT the target 'churn' as X.\n",
    "assert \"churn\" in churn.columns, \"Expected a 'churn' column (0/1).\"\n",
    "\n",
    "X = churn.drop(columns=\"churn\")\n",
    "y = churn[\"churn\"].astype(int)\n",
    "\n",
    "# identify column types\n",
    "possible_cats = [\"gender\", \"plan\", \"topgenre\", \"secondgenre\"]\n",
    "categorical = [c for c in possible_cats if c in X.columns]\n",
    "\n",
    "# treat \"binary\" 0/1 cols as numeric; everything else that's number-like is numeric\n",
    "numeric = [c for c in X.columns if c not in categorical]\n",
    "\n",
    "# build preprocessor to one-hot the categorical and scale numeric where useful\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(with_mean=False), numeric),  # sparse safe scaling\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# === TRAIN / TEST SPLIT (90/10, stratified) ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# === MODELS ===\n",
    "logit = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", LogisticRegression(max_iter=200, n_jobs=None, solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "gb = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": logit,\n",
    "    \"Gradient Boosting\": gb\n",
    "}\n",
    "\n",
    "def eval_model(name, model, Xtr, ytr, Xte, yte):\n",
    "    model.fit(Xtr, ytr)\n",
    "    # predicted probabilities for ROC AUC & calibration\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        p_tr = model.predict_proba(Xtr)[:, 1]\n",
    "        p_te = model.predict_proba(Xte)[:, 1]\n",
    "    else:\n",
    "        # fallback if decision_function exists\n",
    "        p_tr = model.decision_function(Xtr)\n",
    "        p_te = model.decision_function(Xte)\n",
    "\n",
    "    yhat_tr = (p_tr >= 0.5).astype(int)\n",
    "    yhat_te = (p_te >= 0.5).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_acc\": accuracy_score(ytr, yhat_tr),\n",
    "        \"train_prec\": precision_score(ytr, yhat_tr, zero_division=0),\n",
    "        \"train_rec\": recall_score(ytr, yhat_tr, zero_division=0),\n",
    "        \"train_auc\": roc_auc_score(ytr, p_tr),\n",
    "        \"test_acc\": accuracy_score(yte, yhat_te),\n",
    "        \"test_prec\": precision_score(yte, yhat_te, zero_division=0),\n",
    "        \"test_rec\": recall_score(yte, yhat_te, zero_division=0),\n",
    "        \"test_auc\": roc_auc_score(yte, p_te),\n",
    "        \"brier_test\": brier_score_loss(yte, p_te)\n",
    "    }\n",
    "    print(f\"\\n{name} RESULTS\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"TRAIN -> Acc {metrics['train_acc']:.3f} | Prec {metrics['train_prec']:.3f} | \"\n",
    "          f\"Rec {metrics['train_rec']:.3f} | AUC {metrics['train_auc']:.3f}\")\n",
    "    print(f\"TEST  -> Acc {metrics['test_acc']:.3f} | Prec {metrics['test_prec']:.3f} | \"\n",
    "          f\"Rec {metrics['test_rec']:.3f} | AUC {metrics['test_auc']:.3f} | \"\n",
    "          f\"Brier {metrics['brier_test']:.3f}\")\n",
    "    return metrics, p_te, model\n",
    "\n",
    "results = {}\n",
    "probs_test = {}\n",
    "fitted = {}\n",
    "\n",
    "for name, m in models.items():\n",
    "    metrics, p_te, mdl = eval_model(name, m, X_train, y_train, X_test, y_test)\n",
    "    results[name] = metrics\n",
    "    probs_test[name] = p_te\n",
    "    fitted[name] = mdl\n",
    "\n",
    "# pick best by TEST AUC\n",
    "best_name = max(results, key=lambda k: results[k][\"test_auc\"])\n",
    "best_model = fitted[best_name]\n",
    "print(f\"\\nChosen model (by test AUC): {best_name}\")\n",
    "\n",
    "# === CALIBRATION (test set) ===\n",
    "# Show calibration curves in the output cell so you can paste into your report.\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    for name in models.keys():\n",
    "        CalibrationDisplay.from_predictions(\n",
    "            y_test, probs_test[name], n_bins=10, name=name\n",
    "        )\n",
    "    plt.title(\"Calibration Curves (Test)\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Calibration plot skipped:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chosen model to score NEW customers, then find the 200 highest-risk.\n",
    "probs_new = best_model.predict_proba(new_customers)[:, 1]\n",
    "new_scored = new_customers.copy()\n",
    "new_scored[\"pred\"] = probs_new\n",
    "\n",
    "# top 200 high-risk customers\n",
    "top_k = 200 if len(new_scored) >= 200 else len(new_scored)\n",
    "high_risk = new_scored.nlargest(top_k, \"pred\").copy()\n",
    "\n",
    "# === KNN NEIGHBORS ===\n",
    "# As instructed, use age, income, meanhourwatched to compute neighbors.\n",
    "knn_features = [c for c in [\"age\", \"income\", \"meanhourwatched\"] if c in churn.columns]\n",
    "assert len(knn_features) == 3, \"Expected columns age, income, meanhourwatched to exist.\"\n",
    "\n",
    "# Fit neighbors on the FULL CUSTOMER BASE (trainable space = existing churn dataset)\n",
    "scaler_knn = StandardScaler()\n",
    "base_X = scaler_knn.fit_transform(churn[knn_features].values)\n",
    "\n",
    "# Build KNN model (10 neighbors). We'll include more (11) and drop self if overlap happens.\n",
    "k = 10\n",
    "nbrs = NearestNeighbors(n_neighbors=k, metric=\"euclidean\").fit(base_X)\n",
    "\n",
    "# For each high-risk NEW customer, find 10 most similar existing customers.\n",
    "def neighbor_ids_for_row(row):\n",
    "    vec = scaler_knn.transform([row[knn_features].values])\n",
    "    distances, indices = nbrs.kneighbors(vec, n_neighbors=k)\n",
    "    # return the index positions; if your churn DataFrame has a user id column, map it here\n",
    "    # e.g., churn.loc[indices[0], \"user_id\"].tolist()\n",
    "    return indices[0].tolist()\n",
    "\n",
    "high_risk[\"neighbors\"] = high_risk.apply(neighbor_ids_for_row, axis=1)\n",
    "\n",
    "# Show the first few rows like the assignment example (pred + neighbors list)\n",
    "high_risk_display_cols = [c for c in [\n",
    "    \"gender\",\"age\",\"income\",\"monthssubbed\",\"plan\",\"meanhourwatched\",\n",
    "    \"competitorsub\",\"numprofiles\",\"cancelled\",\"downgraded\",\"bundle\",\"kids\",\n",
    "    \"longestsession\",\"topgenre\",\"secondgenre\",\"pred\",\"neighbors\"\n",
    "] if c in high_risk.columns]\n",
    "\n",
    "high_risk[high_risk_display_cols].head(6)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
